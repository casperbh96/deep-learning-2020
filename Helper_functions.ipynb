{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from itertools import repeat\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If using Colab, you may need to upload `plotting.py`.           \n",
      "In the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer           \n",
      "---------------------------------------------\n",
      "No module named 'plotting'\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Normal, OneHotCategorical\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "from itertools import repeat\n",
    "try:\n",
    "    from plotting import make_vae_plots\n",
    "except Exception as ex:\n",
    "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
    "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
    "          \\n---------------------------------------------\")\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(k):\n",
    "    \"\"\"\n",
    "    Converts a number to its one-hot or 1-of-k representation\n",
    "    vector.\n",
    "    :param k: (int) length of vector\n",
    "    :return: onehot function\n",
    "    \"\"\"\n",
    "    def encode(label):\n",
    "        y = torch.zeros(k)\n",
    "        if label < k:\n",
    "            y[label] = 1\n",
    "        return y\n",
    "    return encode\n",
    "\n",
    "def log_sum_exp(tensor, dim=-1, sum_op=torch.sum):\n",
    "    \"\"\"\n",
    "    Uses the LogSumExp (LSE) as an approximation for the sum in a log-domain.\n",
    "    :param tensor: Tensor to compute LSE over\n",
    "    :param dim: dimension to perform operation over\n",
    "    :param sum_op: reductive operation to be applied, e.g. torch.sum or torch.mean\n",
    "    :return: LSE\n",
    "    \"\"\"\n",
    "    max, _ = torch.max(tensor, dim=dim, keepdim=True)\n",
    "    return torch.log(sum_op(torch.exp(tensor - max), dim=dim, keepdim=True) + 1e-8) + max\n",
    "\n",
    "def enumerate_discrete(x, y_dim):\n",
    "    \"\"\"\n",
    "    Generates a `torch.Tensor` of size batch_size x n_labels of\n",
    "    the given label.\n",
    "    Example: generate_label(2, 1, 3) #=> torch.Tensor([[0, 1, 0],\n",
    "                                                       [0, 1, 0]])\n",
    "    :param x: tensor with batch size to mimic\n",
    "    :param y_dim: number of total labels\n",
    "    :return variable\n",
    "    \"\"\"\n",
    "    def batch(batch_size, label):\n",
    "        labels = (torch.ones(batch_size, 1) * label).type(torch.LongTensor)\n",
    "        y = torch.zeros((batch_size, y_dim))\n",
    "        y.scatter_(1, labels, 1)\n",
    "        return y.type(torch.LongTensor)\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    generated = torch.cat([batch(batch_size, i) for i in range(y_dim)])\n",
    "\n",
    "    if x.is_cuda:\n",
    "        generated = generated.cuda()\n",
    "\n",
    "    return Variable(generated.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stochastic(nn.Module):\n",
    "    \"\"\"\n",
    "    Base stochastic layer that uses the\n",
    "    reparametrization trick [Kingma 2013]\n",
    "    to draw a sample from a distribution\n",
    "    parametrised by mu and log_var.\n",
    "    \"\"\"\n",
    "    def reparametrize(self, mu, log_var):\n",
    "        epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
    "\n",
    "        if mu.is_cuda:\n",
    "            epsilon = epsilon.cuda()\n",
    "\n",
    "        # log_std = 0.5 * log_var\n",
    "        # std = exp(log_std)\n",
    "        std = log_var.mul(0.5).exp_()\n",
    "\n",
    "        # z = std * epsilon + mu\n",
    "        z = mu.addcmul(std, epsilon)\n",
    "\n",
    "        return z\n",
    "class GaussianSample(Stochastic):\n",
    "    \"\"\"\n",
    "    Layer that represents a sample from a\n",
    "    Gaussian distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GaussianSample, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.mu = nn.Linear(in_features, out_features)\n",
    "        self.log_var = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        log_var = F.softplus(self.log_var(x))\n",
    "\n",
    "        return self.reparametrize(mu, log_var), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_standard_gaussian(x):\n",
    "    \"\"\"\n",
    "    Evaluates the log pdf of a standard normal distribution at x.\n",
    "    :param x: point to evaluate\n",
    "    :return: log N(x|0,I)\n",
    "    \"\"\"\n",
    "    return torch.sum(-0.5 * math.log(2 * math.pi) - x ** 2 / 2, dim=-1)\n",
    "\n",
    "\n",
    "def log_gaussian(x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Returns the log pdf of a normal distribution parametrised\n",
    "    by mu and log_var evaluated at x.\n",
    "    :param x: point to evaluate\n",
    "    :param mu: mean of distribution\n",
    "    :param log_var: log variance of distribution\n",
    "    :return: log N(x|µ,σ)\n",
    "    \"\"\"\n",
    "    log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
    "    return torch.sum(log_pdf, dim=-1)\n",
    "\n",
    "def log_standard_categorical(p):\n",
    "    \"\"\"\n",
    "    Calculates the cross entropy between a (one-hot) categorical vector\n",
    "    and a standard (uniform) categorical distribution.\n",
    "    :param p: one-hot categorical distribution\n",
    "    :return: H(p, u)\n",
    "    \"\"\"\n",
    "    # Uniform prior over y\n",
    "    prior = F.softmax(torch.ones_like(p), dim=1)\n",
    "    prior.requires_grad = False\n",
    "\n",
    "    cross_entropy = -torch.sum(p * torch.log(prior + 1e-8), dim=1)\n",
    "\n",
    "    return cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder_stats(\n",
    "        x: Tensor = None,\n",
    "        x_hat: Tensor = None,\n",
    "        z: Tensor = None,\n",
    "        y: Tensor = None,\n",
    "        epoch: int = None,\n",
    "        train_loss: List = None,\n",
    "        valid_loss: List = None,\n",
    "        classes: List = None,\n",
    "        dimensionality_reduction_op: Optional[Callable] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    An utility \n",
    "    \"\"\"\n",
    "    # -- Plotting --\n",
    "    f, axarr = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "    # Loss\n",
    "    ax = axarr[0, 0]\n",
    "    ax.set_title(\"Error\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Error')\n",
    "\n",
    "    ax.plot(np.arange(epoch + 1), train_loss, color=\"black\")\n",
    "    ax.plot(np.arange(epoch + 1), valid_loss, color=\"gray\", linestyle=\"--\")\n",
    "    ax.legend(['Training error', 'Validation error'])\n",
    "\n",
    "    # Latent space\n",
    "    ax = axarr[0, 1]\n",
    "\n",
    "    ax.set_title('Latent space')\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "\n",
    "    # If you want to use a dimensionality reduction method you can use\n",
    "    # for example TSNE by projecting on two principal dimensions\n",
    "    # TSNE.fit_transform(z)\n",
    "    if dimensionality_reduction_op is not None:\n",
    "        z = dimensionality_reduction_op(z)\n",
    "\n",
    "    colors = iter(plt.get_cmap('Set1')(np.linspace(0, 1.0, len(classes))))\n",
    "    for c in classes:\n",
    "        ax.scatter(*z[y.numpy() == c].T, c=next(colors), marker='o')\n",
    "\n",
    "    ax.legend(classes)\n",
    "\n",
    "    # Inputs\n",
    "    ax = axarr[1, 0]\n",
    "    ax.set_title('Inputs')\n",
    "    ax.axis('off')\n",
    "\n",
    "    rows = 8\n",
    "    batch_size = x.size(0)\n",
    "    columns = batch_size // rows\n",
    "\n",
    "    canvas = np.zeros((28 * rows, columns * 28))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            idx = i % columns + rows * j\n",
    "            canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = x[idx].reshape((28, 28))\n",
    "    ax.imshow(canvas, cmap='gray')\n",
    "\n",
    "    # Reconstructions\n",
    "    ax = axarr[1, 1]\n",
    "    ax.set_title('Reconstructions')\n",
    "    ax.axis('off')\n",
    "\n",
    "    canvas = np.zeros((28 * rows, columns * 28))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            idx = i % columns + rows * j\n",
    "            canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = x_hat[idx].reshape((28, 28))\n",
    "\n",
    "    ax.imshow(canvas, cmap='gray')\n",
    "\n",
    "    tmp_img = \"tmp_ae_out.png\"\n",
    "    plt.savefig(tmp_img)\n",
    "    plt.close(f)\n",
    "    display(Image(filename=tmp_img))\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    os.remove(tmp_img)\n",
    "\n",
    "\n",
    "def plot_samples(ax, x):\n",
    "    x = x.to('cpu')\n",
    "    nrow = int(np.sqrt(x.size(0)))\n",
    "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid.detach().numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def plot_interpolations(ax, vae):\n",
    "    device = next(iter(vae.parameters())).device\n",
    "    nrow = 10\n",
    "    nsteps = 10\n",
    "    prior_params = vae.prior_params.expand(2 * nrow, *vae.prior_params.shape[-1:])\n",
    "    mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "    pz = Normal(mu, log_sigma.exp())\n",
    "    z = pz.sample().view(nrow, 2, -1)\n",
    "    t = torch.linspace(0, 1, 10, device=device)\n",
    "    zs = t[None, :, None] * z[:, 0, None, :] + (1 - t[None, :, None]) * z[:, 1, None, :]\n",
    "    px = vae.observation_model(zs.view(nrow * nsteps, -1))\n",
    "    x = px.sample()\n",
    "    x = x.to('cpu')\n",
    "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def plot_grid(ax, vae):\n",
    "    device = next(iter(vae.parameters())).device\n",
    "    nrow = 10\n",
    "    xv, yv = torch.meshgrid([torch.linspace(-3, 3, 10), torch.linspace(-3, 3, 10)])\n",
    "    zs = torch.cat([xv[:, :, None], yv[:, :, None]], -1)\n",
    "    zs = zs.to(device)\n",
    "    px = vae.observation_model(zs.view(nrow * nrow, 2))\n",
    "    x = px.sample()\n",
    "    x = x.to('cpu')\n",
    "    x_grid = make_grid(x.view(-1, 1, 28, 28), nrow=nrow).permute(1, 2, 0)\n",
    "    ax.imshow(x_grid)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def plot_2d_latents(ax, qz, z, y):\n",
    "    z = z.to('cpu')\n",
    "    y = y.to('cpu')\n",
    "    scale_factor = 2\n",
    "    batch_size = z.shape[0]\n",
    "    palette = sns.color_palette()\n",
    "    colors = [palette[l] for l in y]\n",
    "\n",
    "    # plot prior\n",
    "    prior = plt.Circle((0, 0), scale_factor, color='gray', fill=True, alpha=0.1)\n",
    "    ax.add_artist(prior)\n",
    "\n",
    "    # plot data points\n",
    "    mus, sigmas = qz.mu.to('cpu'), qz.sigma.to('cpu')\n",
    "    mus = [mus[i].numpy().tolist() for i in range(batch_size)]\n",
    "    sigmas = [sigmas[i].numpy().tolist() for i in range(batch_size)]\n",
    "\n",
    "    posteriors = [\n",
    "        plt.matplotlib.patches.Ellipse(mus[i], *(scale_factor * s for s in sigmas[i]), color=colors[i], fill=False,\n",
    "                                       alpha=0.3) for i in range(batch_size)]\n",
    "    for p in posteriors:\n",
    "        ax.add_artist(p)\n",
    "\n",
    "    ax.scatter(z[:, 0], z[:, 1], color=colors)\n",
    "\n",
    "    ax.set_xlim([-3, 3])\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "\n",
    "def plot_latents(ax, z,y_):\n",
    "    \n",
    "    # z = z.to('cpu')\n",
    "    palette = sns.color_palette()\n",
    "    colors = [palette[l] for l in y_]\n",
    "    z = TSNE(n_components=2).fit_transform(z)\n",
    "    ax.scatter(z[:, 0], z[:, 1], color = colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import *\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display, clear_output\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def make_plot(vae,x,y_,outputs,tmp_img=\"tmp_vae_out.png\",figsize=(16,16)):\n",
    "  fig, axes = plt.subplots(2,3, figsize=figsize, squeeze=False)\n",
    "\n",
    "  # plot the observation\n",
    "  axes[0, 0].set_title(r'Observation $\\mathbf{x}$')\n",
    "  plot_samples(axes[0, 0], x)\n",
    "\n",
    "  # plot the latent samples\n",
    "  try:\n",
    "      z = outputs['z']\n",
    "      z = z.detach().numpy()\n",
    "      if z.shape[1] == 2:\n",
    "          axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
    "          qz = outputs['qz']\n",
    "          plot_2d_latents(axes[0, 1], qz, z, y_)\n",
    "      else:\n",
    "          axes[0, 1].set_title(r'Latent Samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ (t-SNE)')\n",
    "          plot_latents(axes[0, 1], z,y_)\n",
    "  except Exception as e:\n",
    "      print(f\"Could not generate the plot of the latent sanples because of exception\")\n",
    "      print(e)\n",
    "\n",
    "  #plot posterior samples\n",
    "  axes[0, 2].set_title(\n",
    "      r'Reconstruction $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$')\n",
    "  px = outputs['px']\n",
    "#   x_sample = px.sample().to('cpu')\n",
    "  x_sample = px.to('cpu')\n",
    "\n",
    "  plot_samples(axes[0, 2], x_sample)\n",
    "\n",
    "  # plot ELBO\n",
    "  ax = axes[1, 0]\n",
    "  ax.set_title(r'ELBO: $\\mathcal{L} ( \\mathbf{x} )$')\n",
    "  ax.plot(outputs['losses'], label='Training')\n",
    "  ax.plot(outputs['losses_val'], label='Validation')\n",
    "  ax.legend()\n",
    "\n",
    "  # plot KL\n",
    "  ax = axes[1, 1]\n",
    "  ax.set_title(r'$\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$')\n",
    "  ax.plot(outputs['U_list'], label='U')\n",
    "  ax.plot(outputs['L_list'], label='L')\n",
    "  ax.legend()\n",
    "\n",
    "#   # plot NLL\n",
    "#   ax = axes[1, 2]\n",
    "#   ax.set_title(r'$\\log p_\\theta(\\mathbf{x} | \\mathbf{z})$')\n",
    "#   ax.plot(p_x_yz_list, label='Training')\n",
    "#   # ax.plot(validation_data['log_px'], label='Validation')\n",
    "#   ax.legend()\n",
    "\n",
    "#       # display\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(tmp_img)\n",
    "  plt.show()\n",
    "  plt.close(fig)\n",
    "  display(Image(filename=tmp_img))\n",
    "  clear_output(wait=True)\n",
    "\n",
    "#   os.remove(tmp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
